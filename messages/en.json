{
  "AllModel": {
    "loading": "Loading...",
    "waitingAnswer": "Waiting Answer...",
    "start": "Start",
    "title": "Model Arena | awesomeprompt.net",
    "description": "AI Safety Workshop brings you Model Arena, where hundreds of models compete to determine which is stronger, better, and superior.",
    "infoCard": "Model Arena",
    "slogan": "Model Arena, where models compete to determine which is stronger, better, and superior.",
    "view": "View",
    "backHome": "Back Home",
    "noQuestion": "Please enter a question",
    "noModel": "Please select a model",
    "inputQuestion": "Please enter a question",
    "selectModel": "Please select a model",
    "contestModel": "Current Model:",
    "aiJudge": "Model Arena",
    "startNow": "Start Now",
    "whatIsModelJudge": "What is Model Arena",
    "whatIsModelJudgeDesc": "Model Arena is a revolutionary AI model evaluation tool that helps you quickly find the most suitable AI model.",
    "howToUseModelJudge": "How to use Model Arena",
    "howToUseModelJudgeDesc1": "Enter any question or task description",
    "howToUseModelJudgeDesc2": "Select three different AI models from the available model list for testing",
    "howToUseModelJudgeDesc3": "Wait for the selected three models to complete their answers",
    "howToUseModelJudgeDesc4": "The system will automatically call the fourth model (judging model) to evaluate and score the answers of the three models",
    "howToUseModelJudgeDesc5": "View the evaluation results, including the scores and detailed evaluations of each model",
    "howToUseModelJudgeDesc6": "Based on the evaluation results, select the most suitable model for your needs",
    "tip": "Tip:",
    "tipDesc": "To get the best results, please describe your question or task as clearly and specifically as possible. This will help AI models provide more accurate and relevant answers.",
    "whyUseModelJudge": "Why use Model Arena",
    "reduceTrialCost": "Reduce Trial Cost",
    "reduceTrialCostDesc": "With the rise of AI, more and more models have emerged. Users often have no idea how to choose a model. Model Arena helps you quickly find the most suitable model for your needs, saving valuable time and resources.",
    "saveTime": "Save Time",
    "saveTimeDesc": "When choosing a model, users often need to spend a lot of time trying different models. Model Arena allows you to test multiple models at once, quickly obtain results, greatly improving the selection efficiency.",
    "improveEfficiency": "Improve Efficiency",
    "improveEfficiencyDesc": "Model Arena supports up to four models at a time, including models such as Qwen, DeepSeek, Zero-One, and many more. Simply click a few times, and you can get a comprehensive evaluation result immediately.",
    "getAccurateResult": "Get Accurate Results",
    "getAccurateResultDesc": "Model Arena calls a fourth model (judging model) to evaluate and score the answers of the three models, ensuring the accuracy and reliability of the evaluation results.",
    "modelJudgeStandard": "Model Arena Evaluation Standards",
    "modelJudgeStandardDesc": "Model Arena's evaluation is based on a comprehensive assessment of multiple dimensions, including:",
    "modelJudgeStandardDesc1": "Accuracy: Whether the information provided by the model is accurate",
    "modelJudgeStandardDesc2": "Relevance: Whether the model's answer is closely related to the question",
    "modelJudgeStandardDesc3": "Logic: Whether the model's answer has a clear logical structure",
    "modelJudgeStandardDesc4": "Flexibility: Whether the model can handle various types of questions",
    "modelJudgeStandardDesc5": "Completeness: Whether the model's answer is comprehensive and thorough",
    "modelJudgeStandardDesc6": "Practicality: Whether the model's answer has practical application value",
    "modelJudgeStandardDesc7": "This comprehensive evaluation method can help you better understand the strengths and limitations of each model, thereby making informed choices. When choosing a model, you can refer to Model Arena's scores and evaluations to select the most suitable model for your needs.",
    "addModel": "Add Model",
    "minimumModelsRequired": "At least 2 models are required for comparison",
    "modelPrompt": "Enter a question",
    "api": "API",
    "apiWarning": "After setting, please refresh the page to take effect. The key will be stored in your local browser only and will NOT be sent to any server.",
    "evaluationHistory": "Evaluation History",
    "exportResults": "Export Results",
    "copyMarkdown": "Copy as Markdown",
    "downloadCSV": "Download CSV",
    "evaluationScore": "Evaluation Score",
    "clearEvaluationHistory": "Clear History",
    "clearEvaluationHistoryConfirm": "Are you sure you want to clear all evaluation history?",
    "noEvaluationHistory": "No evaluation history found",
    "loadEvaluation": "Load Evaluation",
    "evaluationTime": "Evaluation Time",
    "evaluationDimensions": "Evaluation Dimensions",
    "customizeEvaluationCriteria": "Customize Evaluation Criteria",
    "evaluationCriteriaDesc": "Adjust weight or enable/disable evaluation dimensions",
    "dimensionName": "Dimension Name",
    "weight": "Weight",
    "enabled": "Enabled",
    "description": "Description",
    "resetToDefault": "Reset to Default",
    "advancedResults": "Advanced Results",
    "aiEvaluationPrompt": "Evaluate the following answers to the question: '{question}' and determine which answer is better. For each answer, provide a score and detailed analysis of its strengths and weaknesses.",
    "totalScore": "Total Score",
    "modelAnswerEvaluation": "Model Answer Evaluation",
    "strengths": "Strengths",
    "weaknesses": "Weaknesses",
    "conclusion": "Conclusion",
    "model": "Model",
    "evaluationResult": "Evaluation Result",
    "saveToHistory": "Save to History",
    "projectSupport": "Thanks to ",
    "projectFreeApi": " for providing free API service.",
    "projectOpenSource": "This project is now open source, welcome to visit",
    "cooperation": "",
    "wechat": "",
    "wechatQRCode": "WeChat QR Code",
    "wechatQRCodeDesc": "Welcome to contact us for business cooperation",
    "projectLinks": "Project Links",
    "projectInfo": "Project Info",
    "connectLinks": ""
  },
  "Header": {
    "home": "Home",
    "login": "Login",
    "logout": "Logout"
  },
  "ModelVs": {
    "title": "Model Debate | modeldebate.awesomeprompt.net",
    "description": "A platform where AI models debate with each other, providing you with multiple perspectives and deep insights.",
    "startDebate": "Start Debate",
    "debateTopic": "Debate Topic",
    "debateTopicPlaceholder": "Enter debate topic...",
    "modelA": "Model A",
    "modelB": "Model B",
    "debateRounds": "Debate Rounds",
    "startingPosition": "Starting Position",
    "modelAStarts": "Model A Starts",
    "modelBStarts": "Model B Starts",
    "debating": "Debating...",
    "round": "Round",
    "conclusion": "Conclusion",
    "generateConclusion": "Generate Conclusion",
    "moderator": "Moderator",
    "debateHistory": "Debate History",
    "noDebateHistory": "No debate history found",
    "loadDebate": "Load Debate",
    "debateTime": "Debate Time",
    "clearDebateHistory": "Clear History",
    "clearDebateHistoryConfirm": "Are you sure you want to clear all debate history?",
    "saveToHistory": "Save to History",
    "shareDebate": "Share Debate",
    "copyDebateLink": "Copy Debate Link",
    "debateLinkCopied": "Debate link copied to clipboard!",
    "downloadTranscript": "Download Transcript",
    "defaultDebateTopic": "Is AI more likely to help or harm humanity in the long run?",
    "conclusionGenerated": "Conclusion Generated",
    "aiConclusionPrompt": "Please summarize the debate between two AI models on the topic: '{topic}'. Highlight the key arguments made by both sides, areas of agreement and disagreement, and provide a balanced conclusion that considers the nuances of both perspectives."
  },
  "EvaluationCriteria": {
    "customizeCriteria": "Customize Criteria",
    "customizeCriteriaTitle": "Customize Model Evaluation Criteria",
    "customizeCriteriaDescription": "Adjust evaluation dimensions and their weights to better fit your specific needs",
    "currentCriteria": "Current Evaluation Criteria",
    "totalWeight": "Total Weight",
    "willBeNormalized": "Will be automatically adjusted to 100%",
    "addNewCriterion": "Add New Evaluation Dimension",
    "newCriterionPlaceholder": "Enter new criterion name",
    "add": "Add",
    "resetToDefault": "Reset to Default",
    "cancel": "Cancel",
    "save": "Save",
    "accuracy": "Accuracy",
    "relevance": "Relevance",
    "logic": "Logic",
    "creativity": "Creativity",
    "completeness": "Completeness",
    "practicality": "Practicality",
    "grammar": "Grammar",
    "fluency": "Fluency",
    "style": "Style",
    "innovation": "Innovation",
    "appeal": "Appeal",
    "fairness": "Fairness",
    "safety": "Safety",
    "compliance": "Compliance",
    "information": "Information Value",
    "enabled": "Enabled",
    "disabled": "Disabled",
    "weight": "Weight",
    "remove": "Remove",
    "enableDisable": "Enable/Disable",
    "weightAdjustment": "Weight Adjustment"
  },
  "EvaluationHistory": {
    "viewHistory": "View History",
    "evaluationHistory": "Evaluation History",
    "evaluationHistoryDescription": "View your previous model evaluation results",
    "noHistoryRecords": "No history records",
    "date": "Date",
    "question": "Question",
    "models": "Models",
    "bestModel": "Best Model",
    "actions": "Actions",
    "delete": "Delete",
    "clearAll": "Clear All",
    "export": "Export",
    "close": "Close",
    "showingResultsFromHistory": "Showing results from history",
    "confirmClearAll": "Are you sure you want to clear all history? This action cannot be undone."
  },
  "ConversationEvaluation": {
    "conversationEvaluation": "Conversation Evaluation",
    "conversationEvaluationDescription": "Test how models perform in ongoing conversations",
    "selectModels": "Select Models to Compare",
    "selectModelsDescription": "Choose at least two models for multi-turn conversation comparison",
    "selectAtLeastTwoModels": "Please select at least two models for comparison",
    "startConversation": "Start Conversation",
    "setup": "Setup",
    "conversation": "Conversation",
    "results": "Results",
    "conversationRound": "Round {count} of Conversation",
    "modelsInConversation": "{count} models in conversation",
    "clear": "Clear",
    "evaluate": "Evaluate",
    "model": "Model",
    "user": "User",
    "assistant": "Assistant",
    "typeMessage": "Type a message...",
    "send": "Send",
    "evaluationResults": "Evaluation Results",
    "evaluationResultsDescription": "Performance assessment of each model in the multi-turn conversation",
    "score": "Score",
    "messagesExchanged": "Messages Exchanged",
    "backToConversation": "Back to Conversation",
    "errorFetchingResponse": "Error fetching response",
    "confirmClearConversation": "Are you sure you want to clear the current conversation? This action cannot be undone.",
    "evaluationPrompt": "Please evaluate the performance of the following models in this multi-turn conversation",
    "evaluationInstructions": "For each model, please provide a score (out of 100) and detailed evaluation, including their strengths and weaknesses."
  },
  "Thinker": {
    "title": "Thinker",
    "description": "Reflecting on humanity's past, present, and future"
  },
  "Client": {
    "modelDescription": "Model name can be model name or ID, e.g. fill in gpt-3.5-turbo, non-official API needs to be filled in correctly",
    "modelPrompt": "Prompt",
    "history": "History",
    "regenerate": "Regenerate",
    "judgeTip": "Click the Start Judging button, the AI tester will automatically analyze the answers of the two models and select the better result",
    "resetPromt": "Reset Prompt",
    "inputPrompt": "Leverage the power of AI...",
    "toggleDarkMode": "Toggle Dark Mode",
    "switchLanguage": "Switch Language",
    "startJudge": "Start Judging",
    "judging": "Judging...",
    "judge": "Judge",
    "judge1": "Judging Result",
    "judge2": "Judging Basis",
    "copyToClipboard": "Copy to Clipboard",
    "copy": "Copy Code",
    "question": "Question",
    "send": "Send",
    "loading": "Loading...",
    "processingPrompt": "Processing...",
    "enterYourPrompt": "Enter your prompt...",
    "modelName": "Model Name",
    "languageModel": "Language Model",
    "generateResponse": "Generate Response",
    "model": "Model ",
    "startComparing": "Start Comparing",
    "comparing": "Comparing...",
    "deleteModel": "Delete Model",
    "addModel": "Add Model",
    "atLeastTwoModels": "At least 2 models are required for comparison",
    "maximumModelsReached": "Maximum of 6 models can be added for comparison"
  },
  "Footer": {
    "projectSource": "This project is separated from",
    "projectSupport": "Thanks for",
    "projectFreeApi": "free API service.",
    "projectOpenSource": "The project is currently open source, welcome to visit",
    "cooperation": "Cooperation",
    "wechat": "Wechat",
    "wechatQRCode": "Wechat QR Code",
    "wechatQRCodeDesc": "Welcome to contact us for business cooperation",
    "projectLinks": "Project Links",
    "friendLinks": "Friend Links",
    "connectLinks": "Connect Links",
    "infoCard": "Model Arena"
  }
}
